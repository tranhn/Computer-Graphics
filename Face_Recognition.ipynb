{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import face_recognition\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image and detect face location\n",
    "image = face_recognition.load_image_file('Tran.jpg')\n",
    "face_locations = face_recognition.face_locations(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get face feature\n",
    "face_landmarks_list = face_recognition.face_landmarks(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face(s):  1\n"
     ]
    }
   ],
   "source": [
    "#print number of faces\n",
    "print(\"Face(s): \", len(face_landmarks_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chin in this face has the following points; [(80, 131), (82, 147), (86, 161), (89, 176), (94, 189), (103, 200), (115, 209), (126, 217), (140, 219), (154, 217), (166, 209), (178, 200), (187, 189), (193, 176), (196, 161), (198, 147), (200, 131)]\n",
      "The left_eyebrow in this face has the following points; [(90, 120), (99, 113), (110, 112), (120, 114), (130, 118)]\n",
      "The right_eyebrow in this face has the following points; [(145, 117), (156, 113), (167, 111), (178, 113), (187, 119)]\n",
      "The nose_bridge in this face has the following points; [(138, 132), (138, 142), (138, 153), (137, 163)]\n",
      "The nose_tip in this face has the following points; [(127, 170), (132, 172), (138, 173), (143, 172), (148, 170)]\n",
      "The left_eye in this face has the following points; [(103, 133), (110, 129), (118, 129), (124, 135), (117, 137), (109, 137)]\n",
      "The right_eye in this face has the following points; [(153, 135), (160, 129), (167, 128), (174, 132), (169, 136), (161, 137)]\n",
      "The top_lip in this face has the following points; [(121, 190), (127, 185), (134, 183), (138, 184), (143, 183), (150, 186), (157, 190), (154, 190), (143, 188), (138, 188), (134, 188), (124, 190)]\n",
      "The bottom_lip in this face has the following points; [(157, 190), (151, 196), (144, 198), (138, 199), (134, 199), (127, 196), (121, 190), (124, 190), (134, 191), (138, 191), (143, 191), (154, 190)]\n"
     ]
    }
   ],
   "source": [
    "#load image and draw landmarks\n",
    "pil_image = Image.fromarray(image)\n",
    "d = ImageDraw.Draw(pil_image)\n",
    "\n",
    "for face_landmarks in face_landmarks_list:\n",
    "    for facial_feature in face_landmarks.keys():\n",
    "        print('The {} in this face has the following points; {}'.format(facial_feature, face_landmarks[facial_feature]))\n",
    "    for facial_feature in face_landmarks.keys():\n",
    "        d.line(face_landmarks[facial_feature], width=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show draw image and save it\n",
    "pil_image.show()\n",
    "pil_image.save('face_feature.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get video from webcam\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "#load image of people\n",
    "tran_image = face_recognition.load_image_file('Tran.jpg')\n",
    "face_encoding = face_recognition.face_encodings(tran_image)[0]\n",
    "\n",
    "# legal faces list\n",
    "legal_face_encodings = [\n",
    "    face_encoding,\n",
    "]\n",
    "\n",
    "#legal names list\n",
    "legal_face_names = [\n",
    "    'Tran',\n",
    "]\n",
    "\n",
    "while True:\n",
    "    r, frame = cam.read()\n",
    "    #convert image form BGR (opencv uses) to RGB (face_recognition uses)\n",
    "    rgb_frame = frame[:,:,::-1]\n",
    "    \n",
    "    # find all face\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "    \n",
    "    # loop through each face\n",
    "    for(top, right,bottom,left), face_encoding in zip(face_locations, face_encodings):\n",
    "        matches = face_recognition.compare_faces(legal_face_encodings, face_encoding)\n",
    "        \n",
    "        name = 'Unknown'\n",
    "        \n",
    "        face_distances = face_recognition.face_distance(legal_face_encodings, face_encoding)    \n",
    "        best_match_index = np.argmin(face_distances)\n",
    "        if (matches[best_match_index]):\n",
    "            name = legal_face_names[best_match_index]\n",
    "        cv2.rectangle(frame,(left,top),(right,bottom),(0,0,255),2)\n",
    "        cv2.rectangle(frame,(left,bottom-35),(right,bottom),(0,0,255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame,name,(left+6, bottom-6),font,1.0,(2555,255,255),1)\n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "        break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
